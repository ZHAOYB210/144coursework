{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 1\n",
      "\n",
      "A) For testStr below, write the regular expression `sub` call that replaces all instances of `(sub)` with  `$%\\*$1\\1`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "testStr = \"\"\"Down in my sub, I wanted to\n",
      "submit to something (substantial) that wouldn't\n",
      "lead to subalterns calling in{sub}ordination, or anything else\n",
      "that I could (sub).\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "subRE = re.compile(r'\\(sub\\)') #finds (sub)\n",
      "out =  subRE.sub(r'$%\\\\*$1\\\\1', testStr) #replaces (sub) with $%\\*$1\\1\n",
      "print out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Down in my sub, I wanted to\n",
        "submit to something (substantial) that wouldn't\n",
        "lead to subalterns calling in{sub}ordination, or anything else\n",
        "that I could $%\\*$1\\1.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "B) Now, write another regular expression to substitute `$%\\*$1\\1` with `(sub)`. You'll know you have it right because the output will be identical to `testStr`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "otherFind = re.compile(r'\\$%\\\\\\*\\$1\\\\1') #finds $%\\*$1\\1 with special re characters escaped\n",
      "print otherFind.sub('(sub)', out) == testStr #substitutes $%\\*$1\\1 with (sub) and returns truth value for otherFind.sub == testStr\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "True\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Problem 2\n",
      "\n",
      "In this directory, I have provided the file `romanticism.txt`. Please open it and read the entire contents into one string. You can use the `read()` method of the file object to do that. \n",
      "\n",
      "Now the format of the file is like this:\n",
      "\n",
      "line 1: a title line, containing a title and then some citational material and/or other stuff\n",
      "\n",
      "line 2: blank \n",
      "\n",
      "line 3: the url of the resource\n",
      "\n",
      "line 4: blank\n",
      "\n",
      "line 5: like line 1\n",
      "\n",
      "line 6: like line 2\n",
      "\n",
      "etc.\n",
      "\n",
      "Your task here is to use regular expressions to make these into proper HTML links. In html, links use the **anchor** tag, like so:\n",
      "\n",
      "    <a href=\"url\" target=\"BLANK\">link text</a>\n",
      "\n",
      "Please save your output in the string `output`. As you can see in the following line, I'm using that name to display the output as HTML using the `HTML` function from `IPython.display`. You obviously can change this code to your liking. I'm just trying to get things to print in a way that is easy to read. [More here](http://nbviewer.ipython.org/urls/raw.github.com/ipython/ipython/1.x/examples/notebooks/Part%205%20-%20Rich%20Display%20System.ipynb)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "os.chdir(\"/Users/ibersue/Documents/Computational Methods /144coursework/Homeworks/Homework 4\")\n",
      "os.getcwd()\n",
      "\n",
      "rompath = \"./romanticism.txt\"\n",
      "f = open(rompath, \"r\") \n",
      "b = f.read()\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "fRE = re.compile(r'(?P<title>^\\b[aA-Z0-9].+?(?=[(:,]))(?P<subtitle>[(:,].+[:\\n])|(?P<url>\\b(http://.+)|(gopher://.+\\b))', re.MULTILINE) #check in debuggex? \n",
      "\n",
      "\n",
      "#newRE = re.compile(r'(?P<title>^\\b[aA-Z0-9].+?(?=[\\(:,]))', re.MULTILINE) #find all website titles\n",
      "\n",
      "#can use (?P=url) and (?P=title) to backreference\n",
      "\n",
      "#inputstr = newRE.sub(r'\\g<title>', b)\n",
      "output =  fRE.sub(r'<a href= (|\\g<url>) target= \"BLANK\"> (|\\g<title>)  </a> (|\\g<subtitle>)', b) #currently urls are correct but not titles. Almost to the format he wants.\n",
      "\n",
      "#output = newRE.sub(r'<a href=\\g<url> target= \"BLANK\"> \\g<title>  </a>', outputs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "error",
       "evalue": "unmatched group",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-81-4e02ff5ed5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#inputstr = newRE.sub(r'\\g<title>', b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfRE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<a href= (|\\g<url>) target= \"BLANK\"> (|\\g<title>)  </a> (|\\g<subtitle>)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#currently urls are correct but not titles. Almost to the format he wants.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#output = newRE.sub(r'<a href=\\g<url> target= \"BLANK\"> \\g<title>  </a>', outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ibersue/anaconda/python.app/Contents/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(match, template)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/ibersue/anaconda/python.app/Contents/lib/python2.7/sre_parse.pyc\u001b[0m in \u001b[0;36mexpand_template\u001b[0;34m(template, match)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mliterals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unmatched group\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"invalid group reference\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31merror\u001b[0m: unmatched group"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('Romantic Chronology ', '', '', '', ''), ('', '(UC Santa Barbara & Miami U., Ohio):\\n', '', '', ''), ('', '', 'http://english.ucsb.edu:591/rchrono/', 'http://english.ucsb.edu:591/rchrono/', ''), ('British Women Romantic Poets', '', '', '', ''), ('', ', 1789-1832 (U of Cal. Davis):\\n', '', '', ''), ('', '', 'http://www.lib.ucdavis.edu/English/BWRP/index.htm', 'http://www.lib.ucdavis.edu/English/BWRP/index.htm', ''), ('The European Mirror of the Romantic Chronology ', '', '', '', ''), ('', \"(Queen's U., Belfast):\\n\", '', '', ''), ('', '', 'http://www.qub.ac.uk/english/shuttle/rom-chrono/chrono.htm', 'http://www.qub.ac.uk/english/shuttle/rom-chrono/chrono.htm', ''), ('The Modern English Collection at the Electronic Center ', '', '', '', ''), ('', '(UVa)\\n', '', '', ''), ('', '', 'http://etext.lib.virginia.edu/modeng.browse.html', 'http://etext.lib.virginia.edu/modeng.browse.html', ''), ('British Poetry 1780-1910', '', '', '', ''), ('', ': A Hypermedia Archive of Scholarly Editions (UVA), co-editor, Jerome McGann:\\n', '', '', ''), ('', '', 'http://etext.lib.virginia.edu/britpo.html', 'http://etext.lib.virginia.edu/britpo.html', ''), ('Blake Archive ', '', '', '', ''), ('', '(hosted at UVA), directed by Morris Eaves, Robert N. Essick, and Joseph Viscomi:\\n', '', '', ''), ('', '', 'http://jefferson.village.virginia.edu/blake/', 'http://jefferson.village.virginia.edu/blake/', ''), ('The UPenn Gopher ', '', '', '', ''), ('', '(with its substantial non-canonical Romantics holdings):\\n', '', '', ''), ('', '', 'gopher://dept.english.upenn.edu:70/11/Courses', '', 'gopher://dept.english.upenn.edu:70/11/Courses'), ('also known as PEAL', '', '', '', ''), ('', ': Penn English Archive and Library\\n', '', '', ''), ('', '', 'gopher://dept.english.upenn.edu/11/E-Text/PEAL', '', 'gopher://dept.english.upenn.edu/11/E-Text/PEAL'), ('Romantic Links and Home Pages ', '', '', '', ''), ('', '(Univ.of Penna.), Michael Gamer\\n', '', '', ''), ('', '', 'http://dept.english.upenn.edu/~mgamer/Romantic/', 'http://dept.english.upenn.edu/~mgamer/Romantic/', ''), ('The Frankenstein Project ', '', '', '', ''), ('', '(U. Penn):\\n', '', '', ''), ('', '', 'http://gopher.upenn.edu/pennprintout/html/v11/3/creature.html', 'http://gopher.upenn.edu/pennprintout/html/v11/3/creature.html', ''), ('Romantic Circles Project ', '', '', '', ''), ('', '(Second-Generation Romantics and Their Circle), major participants, Donald Reiman, Neil Fraistat, Carl Stahmer:\\n', '', '', ''), ('', '', 'http://www.rc.umd.edu/', 'http://www.rc.umd.edu/', ''), ('Bluestocking Archive ', '', '', '', ''), ('', '(U. Mass.), Elizabeth Fay:\\n', '', '', ''), ('', '', 'http://fay.english.umb.edu/', 'http://fay.english.umb.edu/', ''), ('Romantic Women Writers Page ', '', '', '', ''), ('', '(Nottingham U., U.K.), Adriana Craciun:\\n', '', '', ''), ('', '', 'http://www.nottingham.ac.uk/~aezacweb/wrew.htm', 'http://www.nottingham.ac.uk/~aezacweb/wrew.htm', ''), ('Romanticism on the Net ', '', '', '', ''), ('', '(Oxford U., England):\\n', '', '', ''), ('', '', 'http://users.ox.ac.uk/~scat0385', 'http://users.ox.ac.uk/~scat0385', ''), ('Romanticsm CD-ROM ', '', '', '', ''), ('', '(U. of Alberta):\\n', '', '', ''), ('', '', 'http://www.ualberta.ca/~dmiall/ROMCDINF.HTM', 'http://www.ualberta.ca/~dmiall/ROMCDINF.HTM', ''), ('Romanticism at UT Austin', '', '', '', ''), ('', ', Computer Writing and Research Lab of UT Austin directed by Daniel Anderson:\\n', '', '', ''), ('', '', 'http://www.en.utexas.edu', 'http://www.en.utexas.edu', ''), ('Women of the Romantic Period ', '', '', '', ''), ('', '(UT Austin), Daniel Anderson and Morri Safran:\\n', '', '', ''), ('', '', 'http://www.cwrl.utexas.edu/~worp', 'http://www.cwrl.utexas.edu/~worp', ''), ('New Books in 19th-Century Studies', '', '', '', ''), ('', '', 'http://www.usc.edu/dept/LAS/english/19c/newbooks.html', 'http://www.usc.edu/dept/LAS/english/19c/newbooks.html', ''), ('The Keats-Shelley Journal Home Page', '', '', '', ''), ('', ', ed. Steven E. Jones\\n', '', '', ''), ('', '', 'http://www.luc.edu/publications/keats-shelley/ksjweb.htm', 'http://www.luc.edu/publications/keats-shelley/ksjweb.htm', ''), ('19th Century British and Irish Authors', '', '', '', ''), ('', ', Mitsuharu Matsuoka (Nagoya University):\\n', '', '', ''), ('', '', 'http://lang.nagoya-u.ac.jp/~matsuoka/19th-authors.html', 'http://lang.nagoya-u.ac.jp/~matsuoka/19th-authors.html', ''), ('Victorian Web Sites', '', '', '', ''), ('', ', Mitsuharu Matsuoka (Nagoya University):\\n', '', '', ''), ('', '', 'http://lang.nagoya-u.ac.jp/~matsuoka/Victorian.html', 'http://lang.nagoya-u.ac.jp/~matsuoka/Victorian.html', '')]\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import HTML\n",
      "HTML(output)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "Romantic Chronology (UC Santa Barbara & Miami U., Ohio):\n",
        "\n",
        "http://english.ucsb.edu:591/rchrono/\n",
        "\n",
        "British Women Romantic Poets, 1789-1832 (U of Cal. Davis):\n",
        "\n",
        "http://www.lib.ucdavis.edu/English/BWRP/index.htm\n",
        "\n",
        "The European Mirror of the Romantic Chronology (Queen's U., Belfast):\n",
        "\n",
        "http://www.qub.ac.uk/english/shuttle/rom-chrono/chrono.htm\n",
        "\n",
        "The Modern English Collection at the Electronic Center (UVa)\n",
        "\n",
        "http://etext.lib.virginia.edu/modeng.browse.html\n",
        "\n",
        "British Poetry 1780-1910: A Hypermedia Archive of Scholarly Editions (UVA), co-editor, Jerome McGann:\n",
        "\n",
        "http://etext.lib.virginia.edu/britpo.html\n",
        "\n",
        "Blake Archive (hosted at UVA), directed by Morris Eaves, Robert N. Essick, and Joseph Viscomi:\n",
        "\n",
        "http://jefferson.village.virginia.edu/blake/\n",
        "\n",
        "The UPenn Gopher (with its substantial non-canonical Romantics holdings):\n",
        "\n",
        "gopher://dept.english.upenn.edu:70/11/Courses\n",
        "\n",
        "also known as PEAL: Penn English Archive and Library\n",
        "\n",
        "gopher://dept.english.upenn.edu/11/E-Text/PEAL\n",
        "\n",
        "Romantic Links and Home Pages (Univ.of Penna.), Michael Gamer\n",
        "\n",
        "http://dept.english.upenn.edu/~mgamer/Romantic/\n",
        "\n",
        "The Frankenstein Project (U. Penn):\n",
        "\n",
        "http://gopher.upenn.edu/pennprintout/html/v11/3/creature.html\n",
        "\n",
        "Romantic Circles Project (Second-Generation Romantics and Their Circle), major participants, Donald Reiman, Neil Fraistat, Carl Stahmer:\n",
        "\n",
        "http://www.rc.umd.edu/\n",
        "\n",
        "Bluestocking Archive (U. Mass.), Elizabeth Fay:\n",
        "\n",
        "http://fay.english.umb.edu/\n",
        "\n",
        "Romantic Women Writers Page (Nottingham U., U.K.), Adriana Craciun:\n",
        "\n",
        "http://www.nottingham.ac.uk/~aezacweb/wrew.htm\n",
        "\n",
        "Romanticism on the Net (Oxford U., England):\n",
        "\n",
        "http://users.ox.ac.uk/~scat0385\n",
        "\n",
        "Romanticsm CD-ROM (U. of Alberta):\n",
        "\n",
        "http://www.ualberta.ca/~dmiall/ROMCDINF.HTM\n",
        "\n",
        "Romanticism at UT Austin, Computer Writing and Research Lab of UT Austin directed by Daniel Anderson:\n",
        "\n",
        "http://www.en.utexas.edu\n",
        "\n",
        "Women of the Romantic Period (UT Austin), Daniel Anderson and Morri Safran:\n",
        "\n",
        "http://www.cwrl.utexas.edu/~worp\n",
        "\n",
        "New Books in 19th-Century Studies:\n",
        "\n",
        "http://www.usc.edu/dept/LAS/english/19c/newbooks.html\n",
        "\n",
        "The Keats-Shelley Journal Home Page, ed. Steven E. Jones\n",
        "\n",
        "http://www.luc.edu/publications/keats-shelley/ksjweb.htm\n",
        "\n",
        "19th Century British and Irish Authors, Mitsuharu Matsuoka (Nagoya University):\n",
        "\n",
        "http://lang.nagoya-u.ac.jp/~matsuoka/19th-authors.html\n",
        "\n",
        "Victorian Web Sites, Mitsuharu Matsuoka (Nagoya University):\n",
        "\n",
        "http://lang.nagoya-u.ac.jp/~matsuoka/Victorian.html\n",
        "\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 82,
       "text": [
        "<IPython.core.display.HTML at 0x103479850>"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 3\n",
      "\n",
      "The file `ssns.txt` contains a raw text grab of the geneology website [SortedByName](http://sortedbyname.com). You can have a look. Your job in this problem is to convert that raw list of text into a list of dictionaries, of the following sort:\n",
      "\n",
      "    {\"last\": \"Abrams\", \"first\": \"Bobby\", \"born\": \"23 December 1929\", \"died\": \"03 September 2003\", \"ssn\": \"231-28-2235\"}\n",
      "    \n",
      "Note that as someone without a middle name, I don't care to store them."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "def skip(line):\n",
      "    \"\"\"this is a series of statements about what kind of lines have junk in them\"\"\"\n",
      "    return 0\n",
      "\n",
      "ssnspath = \".//ssns.txt\"\n",
      "f = open(ssnspath, \"r\") \n",
      "b = f.read()#open file, set it to f\n",
      "\n",
      "name = re.compile(r'(?P<name>[A-Z]+,\\s[A-Z]+)')  #finds capital letters followed by a comma, a space, and more capital letters\n",
      "born = re.compile(r'(?P<born>\\bborn\\s\\d{2}\\s\\w+\\s\\d{4})') #finds born followed by 2 numbers (the day), a space, word characters, and 4 more numbers (the year)\n",
      "died = re.compile(r'(?P<died>\\bdied\\s\\d{2}\\s\\w+\\s\\d{4})')#finds died followed by 2 numbers (the day), a space, word characters, and 4 more numbers (year)\n",
      "ssn = re.compile(r'(?P<ssn>\\d{3}-?\\d{2}-?\\d{4})') #finds 3 digitis followed by a dash followed by 2 more digits, another dash, and then 4 more digits\n",
      "         \n",
      "results = []\n",
      "for li in f:\n",
      "    if skip(li):\n",
      "        continue\n",
      "    newPerson = {}\n",
      "\n",
      "    match = name.search(li)\n",
      "    if match: #it found something\n",
      "        newPerson[\"first\"] = name.groups()[0]\n",
      "        newPerson[\"last\"] = name.groups()[1]\n",
      "        newPerson[\"born\"] = born.groups()[1:]\n",
      "        newPerson[\"ssn\"] = ssn.groups()[0]\n",
      "        newPerson[\"died\"] = ssn.groups()[1:]\n",
      "\n",
      "    #etc.\n",
      "    \n",
      "    newPerson.append\n",
      "    results.append(newPerson) #trying to add newPerson to results\n",
      "\n",
      "print results\n",
      "\n",
      "#print b\n",
      "\n",
      "#name.findall(b)     #these demonstrate that all of my regexes work fine\n",
      "#born.findall(b)\n",
      "#died.findall(b)\n",
      "#ssn.findall(b)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 4\n",
      "\n",
      "Write function `addHTMLTag` so that it wraps every instance of `word` in `string` with tag `tag`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def addHTMLbold(word, string, tag):\n",
      "    \"\"\"this looks for word in string, and wraps all occurrences with tag and end_tag\"\"\"\n",
      "    wordRE = re.compile(r'(?P<word>\\b\\w+\\b)')\n",
      "    # you will need to set up some stuff here...\n",
      "    \n",
      "    #loop through the matches to the string one at a time\n",
      "    for m in wordRE.finditer(string):\n",
      "        found = string[m.start():m.end()]\n",
      "        \n",
      "        #now use a format to get the tagged text\n",
      "        wordRE.sub(r'<b>\\g<word></b>', string)\n",
      "        \n",
      "        #and add the tagged part\n",
      "\n",
      "    #here, don't forget to add the start of the string, the end, and whatever is in between the matches\n",
      "    return string #change this!!!!\n",
      "\n",
      "mod = addHTMLbold(\"test\", \"This is a test testy to see what testy retests test out\", \"em\")\n",
      "HTML(mod)\n",
      "mod2 = addHTMLbold(\"is\", mod, \"strong\")\n",
      "HTML(mod2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "This is a test testy to see what testy retests test out"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 54,
       "text": [
        "<IPython.core.display.HTML at 0x103438510>"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Problem 5\n",
      "\n",
      "A) Write a function `splitWithHTML` that tokenizes a text string on HTML tags, keeping the HTML tags in the tokens"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def splitWithHTML(text):\n",
      "    vs =re.split(r'SEARCH', text)\n",
      "    print vs\n",
      "\n",
      "text = \"\"\"\n",
      "<html>\n",
      "<head><title>This is the title</title>\n",
      "<body onload=\"javascript blah blah>>\">This is what <b>is happening <i> to us</i></b></body>\n",
      "<html>\"\"\"\n",
      "\n",
      "splitWithHTML(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\\n', '<html>', '\\n', '<head>', '', '<title>', 'This is the title', '</title>', '\\n', '<body onload=\"javascript blah blah>>\">', 'This is what ', '<b>', 'is happening ', '<i>', ' to us', '</i>', '', '</b>', '', '</body>', '\\n', '<html>', '']\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Problem 6\n",
      "\n",
      "The file `roster.txt` contains the 144 class roster, essentially as a receive it from AIS. Please rewrite each line to show the following elements in the following order: \n",
      "\n",
      "    email address[tab]last name[tab]major1\n",
      "   \n",
      "Note that for the tabs, you should know that the way to find/print them is `\\t`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#there are three basic patterns to search for; do them each\n",
      "splitRE = re.compile(r'(?:PATTERN1|PATTERN2|PATTERN3)')\n",
      "#open file as f\n",
      "for li in f:\n",
      "    # use split\n",
      "    \n",
      "    # and print\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "crjimene@ucsc.edu\tRenee\tLINGBA\n",
        "mikovach@ucsc.edu\tIlona\tLANGBA\n",
        "scweiser@ucsc.edu\tClaire\tPHYSBS\n",
        "jahuizar@ucsc.edu\tAnita\tLINGBA\n",
        "matitone@ucsc.edu\tAnthony\tLINGBA\n",
        "srubenki@ucsc.edu\tRae\tLANGBA\n",
        "mcmata@ucsc.edu\tChristopher\tLINGBA\n",
        "vvanegas@ucsc.edu\tValentina\tLINGBA\n",
        "wmbuchan@ucsc.edu\tMichael\tLINGBA\n",
        "lrchan@ucsc.edu\tRebecca\tLINGBA\n",
        "sfsander@ucsc.edu\tFrancine\tLINGBA\n",
        "shmbianc@ucsc.edu\tMichael\tLINGBA\n",
        "sewinn@ucsc.edu\tEugene\tLINGBA\n",
        "skano@ucsc.edu\t\tLINGBA\n",
        "eswong@ucsc.edu\tStanley Gordon\tLINGBA\n",
        "tcorcora@ucsc.edu\tChase\tLINGBA\n",
        "ksheets@ucsc.edu\tAnne\tLINGBA\n",
        "arzavala@ucsc.edu\tRandall\tLINGBA\n",
        "sclothie@ucsc.edu\tLabou\tLINGBA\n",
        "neggert@ucsc.edu\tWilliam\tPHILBA\n",
        "rastle@ucsc.edu\tNicolle\tLINGBA\n",
        "chanmill@ucsc.edu\tAnn\tLINGBA\n",
        "sneace@ucsc.edu\tRachelle\tLINGBA\n",
        "stgarcia@ucsc.edu\tTrevino\tLINGBA\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 7\n",
      "\n",
      "I have extracted the first few paragraphs of [the following](http://www.latimes.com/business/la-fi-obamacare-oversold-20131030,0,558878.story#axzz2jBmVwJij) and put it in `healthcare.txt`. Your job is to split this into sentences; I have `healthcare-sentences.txt` as an answer key.\n",
      "\n",
      "For this, you probably want to find the boundaries and then merge them backwards. Another approach is to use a bunch of lookbehind tests. Unfortunately, you can't combine them into one big lookbehind because Python requires lookahead/behind to be constant width patterns. In this case, you'll need two different lookarounds.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#open file and read in contents as one string\n",
      "\n",
      "#use split\n",
      "sentenceBoundaries = re.compile(r'(PATTERN)')\n",
      "\n",
      "#and then put the sentence boundary back together with the preceding element"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "As the pitchman for his landmark healthcare law, President Obama promised to make buying insurance as easy as buying a plane ticket online or a \"TV on Amazon.\"\n",
        " \n",
        "It would be simple, he said.\n",
        " \n",
        "If there were problems, the president predicted, they would be \"glitches.\"\n",
        " \n",
        "And he said, \"If you like the plan you have, you can keep it.\"\n",
        " \n",
        "Such claims have come back to haunt the president and his allies less than a month into the launch of the online insurance marketplaces at the heart of his healthcare legislation.\n",
        " \n",
        "With the federal website hobbled by bad design and thousands of policyholders receiving cancellation notices, Obama's promises are not being met \u2014 prompting charges of deception from some Republicans and concessions from some allies that elements of the law were oversold.\n",
        " \n",
        "The fallout is only the latest chapter in this White House's three-year struggle to sell the public on the Affordable Care Act, which could come to define the president's legacy.\n",
        " \n",
        "Since signing it into law, the president has variously defended it, promoted it, simplified it and hyped it.\n",
        " \n",
        "But polling shows he has never fully sold, nor educated, the public on the vast new government healthcare program.\n",
        " \n",
        "Publicly, the White House continued Tuesday to defend the president's pre-launch salesmanship.\n",
        " \n",
        "\"The purpose here wasn't to do anything beyond encourage people to make themselves aware of the options available to them,\" White House Press Secretary Jay Carney said.\n",
        " \n",
        "Behind closed doors, some officials who worked on the rollout say they wish they'd left themselves a little wiggle room.\n",
        " \n",
        "They could have done more to play up ways to sign up other than through the website, such as the call centers, said one official, requesting anonymity to discuss the planning process.\n",
        " \n",
        "After taking heat from allies for not finding a crisp way to explain the complex law, the White House tried to boil it down to its simplest elements, the official said, and some nuance was inevitably lost.\n",
        " \n",
        "The first administration official to testify before Congress on the matter apologized Tuesday, saying, \"the website has not worked as well as it should.\"\n",
        " \n",
        "\"This initial experience has not lived up to our expectations or the expectations of the American people, and it is not acceptable,\" Marilyn Tavenner, the head of the federal Centers for Medicare and Medicaid Services, told the House Ways and Means Committee.\n",
        " \n",
        "Tavenner's agency oversaw the development of the site, intended to link consumers with affordable private health insurance plans that meet standards outlined in the 2010 law.\n",
        " \n",
        "Tavenner vowed that the site would be running smoothly by the end of November, in time for consumers to enroll before the new year.\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 8\n",
      "\n",
      "find how many times a regular verb is used in a text, organized by tense (To be completed)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}