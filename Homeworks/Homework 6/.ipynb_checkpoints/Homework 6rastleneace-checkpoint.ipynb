{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 1\n",
      "\n",
      "Please find all of the control predicates in the WSJ subpart of the Penn Treebank. I recommend looking for predicates you know are control explicitly and using that to build tree patterns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(t)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 2\n",
      "\n",
      "Please produce a table showing, for the top 50 verbs in WSJ subpart of the Penn Treebank, how many times each verb appears in the active, the passive without a by phrase, and the passive with a by phrase. \n",
      "\n",
      "For example, the table might have a line like the following (after the header):\n",
      "\n",
      "<table><tr><th>Verb</th><th>Active</th><th>Passive No By</th><th>Passive By</th></tr>\n",
      "\n",
      "<tr><th>sing</th><th>23</th><th>12</th><th>56</th></tr>\n",
      "</table>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from runTregex import Treebank\n",
      "from runTregex import SearchTree\n",
      "\n",
      "pattern = \"(sing|sang|sung|singing >: /VB*/)\"\n",
      "dir = \"/Users/ibersue/Documents/Computational Methods /144coursework/Homeworks/Homework 6/treebank_3/parsed/mrg/wsj\"\n",
      "\n",
      "t = Treebank(dir,pattern)\n",
      "t.run()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Running tregex...\n",
        "Processing output..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Done!\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freqDict = {} \n",
      "for i in result.matchTree: \n",
      "    if i not in freqDict:             \n",
      "        freqDict[i] = { i+1: 1 } \n",
      "    elif i+1 in freqDict[i]:\n",
      "        freqDict[i][i+1] += 1 \n",
      "    else:\n",
      "        freqDict[i][i+1] = 1\n",
      "return freqDict\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'list' object has no attribute 'matchTree'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-28-9cc4f2e39f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfreqDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#begin a new dictionary for the phonemes in a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatchTree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqDict\u001b[0m\u001b[0;34m:\u001b[0m               \u001b[0;31m#if word item (phoneme) is not yet in the dectionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mfreqDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m}\u001b[0m \u001b[0;31m#it's value is 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfreqDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'matchTree'"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 3\n",
      "\n",
      "Build a subclass of the `SearchTree` class in `runTregex.py` that contains a method `adjacent(self, elt)`, which checks whether adjacent to `self` is a treelet bearing the `elt` as node label (for non-terminals) or value (in the case of leaves)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center><h2>Annotation Exercises<h2></center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 4\n",
      "\n",
      "Using `nltk.download` please download the movie reviews corpus. For the first five neg (cv000_29416.txt to cv005_29357.txt) and first five pos reviews (cv000_29590.txt to cv005_29443.txt), please go through and mark up the following. To do this, you will be wrapping bits of text in the tags below.\n",
      "\n",
      "1. Actor names: Mark each actor in the film with `<A />`\n",
      "Please mark each actor every time they are mentioned and include as much of the name as is continguously mentioned at one time.\n",
      "\n",
      "2. Character names: Mark each character in the film with  `<C />`\n",
      "Please mark each character every time they are mentioned and include as much of the name as is continguously mentioned at one time.\n",
      "\n",
      "3. Movie Name `<M />`\n",
      "This is the name of the movie being reviewed.\n",
      "\n",
      "4. Other movie names `<O />`\n",
      "Other movies are sometimes referenced in a review. Mark all instances.\n",
      "\n",
      "In addition, per sentence (i.e., line) please mark the overall purpose of the sentence with respect to the types below.\n",
      "\n",
      "1. Plot description [p]\n",
      "The central purpose is to describe the structure of the movie's plot, not to comment on it.\n",
      "\n",
      "2. Evaluation [e]\n",
      "The central purpose is to provide an opinion or evaluation. This does not have to be about the movie directly.\n",
      "\n",
      "3. Ground for evaluation [g]\n",
      "This category indicates the reasons for an evaluation. \n",
      "\n",
      "4. Other [o]\n",
      "None of the above\n",
      "\n",
      "In cases where a sentence has multiple types, choose the one that seems most central to the sentence. `Please see exampleAnnotation.txt`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NLTK Downloader\n",
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloader> d \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Download which package (l=list; x=cancel)?\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  Identifier> l\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Packages:\n",
        "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  [ ] abc................. Australian Broadcasting Commission 2006\n",
        "  [ ] alpino.............. Alpino Dutch Treebank\n",
        "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
        "                           Extraction Systems in Biology)\n",
        "  [ ] brown............... Brown Corpus\n",
        "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
        "  [ ] cess_cat............ CESS-CAT Treebank\n",
        "  [ ] cess_esp............ CESS-ESP Treebank\n",
        "  [ ] chat80.............. Chat-80 Data Files\n",
        "  [ ] city_database....... City Database\n",
        "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
        "  [ ] comtrans............ ComTrans Corpus Sample\n",
        "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
        "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
        "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
        "                           and Basque Subset)\n",
        "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
        "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
        "                           Corpus\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] floresta............ Portuguese Treebank\n",
        "  [ ] framenet_v15........ FrameNet 1.5\n",
        "  [ ] gazetteers.......... Gazeteer Lists\n",
        "  [ ] genesis............. Genesis Corpus\n",
        "  [ ] gutenberg........... Project Gutenberg Selections\n",
        "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
        "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
        "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
        "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
        "                           ChaSen format)\n",
        "  [ ] kimmo............... PC-KIMMO Data Files\n",
        "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
        "  [ ] langid.............. Language Id Corpus\n",
        "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
        "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
        "                           part-of-speech tags\n",
        "  [ ] machado............. Machado de Assis -- Obra Completa\n",
        "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
        "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
        "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
        "  [ ] nps_chat............ NPS Chat\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] oanc_masc........... Open American National Corpus: Manually\n",
        "                           Annotated Sub-Corpus\n",
        "  [ ] paradigms........... Paradigm Corpus\n",
        "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
        "                           Evaluation Shared Task\n",
        "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
        "  [ ] pl196x.............. Polish language of the XX century sixties\n",
        "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
        "  [ ] problem_reports..... Problem Report Corpus\n",
        "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
        "  [ ] ptb................. Penn Treebank\n",
        "  [ ] qc.................. Experimental Data for Question Classification\n",
        "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
        "                           version\n",
        "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
        "  [ ] semcor.............. SemCor 3.0\n",
        "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
        "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
        "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
        "  [ ] smultron............ SMULTRON Corpus Sample\n",
        "  [ ] state_union......... C-Span State of the Union Address Corpus\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] stopwords........... Stopwords Corpus\n",
        "  [ ] swadesh............. Swadesh Wordlists\n",
        "  [ ] switchboard......... Switchboard Corpus Sample\n",
        "  [ ] timit............... TIMIT Corpus Sample\n",
        "  [ ] toolbox............. Toolbox Sample Files\n",
        "  [ ] treebank............ Penn Treebank Sample\n",
        "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
        "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
        "                           (Unicode Version)\n",
        "  [ ] unicode_samples..... Unicode Samples\n",
        "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
        "  [ ] webtext............. Web Text Corpus\n",
        "  [ ] wordnet............. WordNet\n",
        "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
        "  [ ] words............... Word Lists\n",
        "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
        "                           English Prose\n",
        "  [ ] basque_grammars..... Grammars for Basque\n",
        "  [ ] book_grammars....... Grammars from NLTK Book\n",
        "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
        "                           for parser comparison\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Hit Enter to continue: \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  [ ] sample_grammars..... Sample Grammars\n",
        "  [ ] spanish_grammars.... Grammars for Spanish\n",
        "  [ ] tagsets............. Help on Tagsets\n",
        "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
        "                           Portuguesa)\n",
        "  [ ] hmm_treebank_pos_tagger Treebank Part of Speech Tagger (HMM)\n",
        "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
        "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
        "  [ ] punkt............... Punkt Tokenizer Models\n",
        "\n",
        "Collections:\n",
        "  [ ] all-corpora......... All the corpora\n",
        "  [ ] all................. All packages\n",
        "  [ ] book................ Everything used in the NLTK Book\n",
        "\n",
        "([*] marks installed packages)\n",
        "\n",
        "Download which package (l=list; x=cancel)?\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  Identifier> movie_reviews\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    Downloading package 'movie_reviews' to /Users/ibersue/nltk_data...\n",
        "      Unzipping corpora/movie_reviews.zip."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "---------------------------------------------------------------------------\n",
        "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
        "---------------------------------------------------------------------------\n"
       ]
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloader> q\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Problem 5\n",
      "\n",
      "Now please come up with some distinction (2-5 labels) that you think might be interesting or important in the movie data. Construct a codebook, including examples for each category and a discussion of problem areas. Annotate the 10 movie reviews with your coding scheme. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}